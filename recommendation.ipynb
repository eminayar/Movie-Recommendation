{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "import os\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class extends python's HTMLParser built-in library. When a text is fed to this parser, handle_starttag is called whenever the parser founds a start tag, handle_endtag is called whenever the parser founds an end tag and handle_data is called whenever a string is found other than a tag component. By inspection of the HTML files, titles are enclosed by title_wrapper tag, storyline is enclosed by a *Storyline* followd by a *span* tag. Recommended movie id's are in a tag called *rec_item*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.title_wrapper_position = -1\n",
    "        self.title = \"\"\n",
    "        self.story = \"\"\n",
    "        self.storyline = False\n",
    "        self.get_story = False\n",
    "        self.recommended = []\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'div' and len(attrs) > 0 and attrs[0][0] == 'class' and attrs[0][1] == 'title_wrapper':\n",
    "            self.title_wrapper_position = self.getpos()[0]\n",
    "        if tag == 'span' and len(attrs) == 0 and self.storyline:\n",
    "            self.get_story = True\n",
    "        if tag == 'div' and len(attrs) == 4 and attrs[0][0] == 'class' and attrs[0][1] == 'rec_item':\n",
    "            self.recommended.append(attrs[3][1])\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'span' and self.storyline:\n",
    "            self.storyline = False\n",
    "            self.get_story = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.title_wrapper_position != -1 and self.getpos()[0] >= self.title_wrapper_position:\n",
    "            if data.strip() == \"\":\n",
    "                return\n",
    "            self.title = data.strip()\n",
    "            self.title_wrapper_position = -1\n",
    "        if data.strip() == 'Storyline':\n",
    "            self.storyline = True\n",
    "        if self.get_story:\n",
    "            if data.strip() == \"\":\n",
    "                return\n",
    "            self.story = self.story + \" \" + data.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pre processing part, I used the same stopwords and punctuations in the first project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = {'#', '[', '~', '-', ']', '.', '@', '/', \"'\", '{', '|', ')',\n",
    "                '(', '*', ',', '`', ';', '$', '%', '\\\\', '^', '_', '!', '<', ':', '&', '>', '\"', '}', '=', '?', '+'}\n",
    "stopwords = {'us', 'for', 'this', 'by', 'few', 'which', 'of', 'why', 'you', 'there', 'them', 'some', 'your', 'her', 'many', 'it', 'will', 'the', 'are', 'all', 'who', 'none', 'they', 'a', 'him', 'an',\n",
    "             'i', 'where', 'its', 'what', 'as', 'have', 'in', 'his', 'she', 'my', 'be', 'any', 'been', 'how', 'or', 'and', 'me', 'their', 'but', 'on', 'is', 'here', 'our', 'with', 'when', 'that', 'was', 'he'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Scraping\n",
    "In this function request the html content using the requests library. Then I feed the html into the html parser. It finds the id,title,story and recommended movies and sets the objects parameters accordingly. Finally I close the parser and return the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_contents(imdb_id):\n",
    "    \"\"\"\n",
    "    Gets an imdb id and returns its title, storyline, list of IMDB recommendations respectively.\n",
    "    \"\"\"\n",
    "    r = requests.get(\"https://www.imdb.com/title/\"+imdb_id)\n",
    "    parser = MyHTMLParser()\n",
    "    parser.feed(r.text)\n",
    "    ret = [imdb_id, parser.title, parser.story, parser.recommended]\n",
    "    parser.close()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates over the given movie id list, collects the data and writes into *\"movie_info.txt\"*. This function is only called(below) if *\"movie_info.txt\"* does not exist. By this way if you run the whole code again, there is no need to make requests again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMDB_scrap():\n",
    "    count = 0\n",
    "    w_file = open(\"movie_info.txt\", \"w\")\n",
    "    for line in open(\"movie_ids.csv\"):\n",
    "        count += 1\n",
    "        print( count , end='\\r')\n",
    "        contents = get_movie_contents(line.strip())\n",
    "        w_file.write(contents[0]+\"\\n\")\n",
    "        w_file.write(contents[1]+\"\\n\")\n",
    "        w_file.write(contents[2]+\"\\n\")\n",
    "        for rec in contents[3]:\n",
    "            w_file.write(rec + \" \")\n",
    "        w_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap only if we haven't scrapped yet. If working it prints the number of scrapped movies so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"movie_info.txt\"):\n",
    "    IMDB_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize global sets,lists and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "token_to_tokenId = {}\n",
    "token_counter = 0\n",
    "movies = []\n",
    "statistic = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize function takes a string, replaces punctuations with space, applies lowercase folding, removes the stopwords from the string and then returns all the tokens in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input):\n",
    "    input = input.strip()\n",
    "    without_punc = \"\"\n",
    "    for char in input:\n",
    "        if char in punctuations:\n",
    "            without_punc += ' '\n",
    "        else:\n",
    "            without_punc += char\n",
    "    tokens = list(map(lambda x: x.lower(), without_punc.split()))\n",
    "    tokens = list(filter(lambda x: x not in stopwords, tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This objects holds information about movies. It has id which is the same as the imdb id, vector which is the tf_idf vector and recommended is the recommended movie ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Movie():\n",
    "    def __init__(self, id):\n",
    "        self.id = id;\n",
    "        self.vector = []\n",
    "        self.recommended = []\n",
    "    \n",
    "    #insert a word and it's term frequency to the tf_idf vector\n",
    "    #note that this function only inserts the term frequency of a containing word.\n",
    "    def insert_element(self, word, freq): \n",
    "        self.vector.append((word, freq))\n",
    "\n",
    "    def insert_rec(self, id): #insert a movie to the recommended movie list\n",
    "        self.recommended.append(id)\n",
    "    \n",
    "    #this function calculates the tf_idf values of the vector. It contained only the term frequency beforehand.\n",
    "    #tf_idf = (1+log10(tf))*log10(N/df)\n",
    "    def tf_idf(self, N):\n",
    "        total_sum = 0.0\n",
    "        for i in range(len(self.vector)):\n",
    "            word = int(self.vector[i][0])\n",
    "            freq = int(self.vector[i][1])\n",
    "            self.vector[i] = (word, (1+math.log10(freq))*math.log10(N/vocabulary[word]) )\n",
    "            total_sum += self.vector[i][1]\n",
    "        self.vector.sort(key = lambda tup: tup[1])\n",
    "        self.vector.reverse() #sort according to tf_idf values\n",
    "        \n",
    "        #here we calculate a statistic to find a value to take the top K element of the tf_idf vectors.\n",
    "        #We find the K value for the current movie which has the 95% of the data(tf_idf values).\n",
    "        sum = 0.0\n",
    "        for i in range(len(self.vector)):\n",
    "            sum += self.vector[i][1]\n",
    "            if sum >= 0.95*total_sum:\n",
    "                statistic.append(i)\n",
    "                break\n",
    "    \n",
    "    # Take the top N element of the tf_idf vector to represent the movie\n",
    "    def take_top_N(self, N):\n",
    "        self.vector = self.vector[:min(N,len(self.vector))]\n",
    "        self.vector.sort(key = lambda tup: tup[0])\n",
    "\n",
    "    #normalize the tf_idf vector. \n",
    "    def normalize(self):\n",
    "        length = 0.0\n",
    "        for elem in self.vector:\n",
    "            length += elem[1]*elem[1]\n",
    "        length = math.sqrt(length)\n",
    "        for i in range(len(self.vector)):\n",
    "            self.vector[i] = ( self.vector[i][0] , self.vector[i][1]/length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scrapping and dumping the data into a file, read it and and create our objects which represents movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file():\n",
    "    global token_counter\n",
    "    count = 0\n",
    "    for line in open(\"movie_info.txt\", \"r\"):\n",
    "        line = line.strip()\n",
    "        if count == 0:\n",
    "            movie = Movie(line)\n",
    "            tf = {}\n",
    "        if count == 1 or count == 2:\n",
    "            for token in tokenize(line):\n",
    "                if token not in token_to_tokenId:\n",
    "                    token_counter += 1\n",
    "                    token_to_tokenId[token] = token_counter\n",
    "                if token_to_tokenId[token] not in tf:\n",
    "                    tf[token_to_tokenId[token]] = 0    \n",
    "                tf[token_to_tokenId[token]] += 1\n",
    "        if count == 3:\n",
    "            for rec in line.split(\" \"):\n",
    "                movie.insert_rec(rec)\n",
    "            for term in tf:\n",
    "                movie.insert_element(int(term), int(tf[term]))\n",
    "                if term not in vocabulary:\n",
    "                    vocabulary[term] = 0\n",
    "                vocabulary[term] += 1\n",
    "            movies.append(movie)\n",
    "        count = (count+1)%4\n",
    "read_data_from_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the *tf_idf* values of the movies, then using the collected statistic find the N value to take top N elements, and then take the top N values of each *tf_idf* vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movies:\n",
    "    movie.tf_idf(len(movies))\n",
    "\n",
    "average = 0\n",
    "for st in statistic:\n",
    "    average += st/len(statistic)\n",
    "for movie in movies:\n",
    "    movie.take_top_N(int(average))\n",
    "    movie.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the cosine similarity of given 2 movies. Since they may have different words which represent it's *tf_idf* vector, we must be careful when to multiply the values( only multiply the values for the same words )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_similarity( movie1, movie2 ):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    similarity = 0.0\n",
    "    while i < len(movie1.vector) and j < len(movie2.vector):\n",
    "        if movie1.vector[i][0] == movie2.vector[j][0]:\n",
    "            similarity += movie1.vector[i][1]*movie2.vector[j][1]\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif movie1.vector[i][0] < movie2.vector[j][0]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "Iterate over all movies. Find the similarities between all movies. Sort them according to similarity and then take the top 11 element(since we only evaluate performance for K=1,2,3,10 we don't need more elements). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(imdb_id):\n",
    "    \"\"\"\n",
    "    Gets an imdb id and returns a list of recommended movie ids for that movie. \n",
    "    \"\"\"\n",
    "    for movie in movies:\n",
    "        if movie.id == imdb_id:\n",
    "            current_movie = movie\n",
    "    recommendations = []\n",
    "    for movie in movies:\n",
    "        if movie.id == current_movie.id:\n",
    "            continue\n",
    "        similarity = calc_similarity(movie , current_movie )\n",
    "        recommendations.append((movie.id,similarity))\n",
    "    recommendations.sort(key = lambda tup: tup[1])\n",
    "    recommendations.reverse()\n",
    "    recommendations = recommendations[:11]\n",
    "    return [x for (x,y) in recommendations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Self explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(rec_movie_ids, relevant_movie_ids, K):\n",
    "    \"\"\"\n",
    "    Gets list of recommended and relevant movie ids and K value.\n",
    "    \n",
    "    Returns precision, recall, F1 values for K respectively. \n",
    "    \"\"\"\n",
    "    rec_movie_ids=rec_movie_ids[:K]\n",
    "    presicion = len(list(filter( lambda x: x in relevant_movie_ids , rec_movie_ids )))/len(rec_movie_ids)\n",
    "    recall = len(list(filter( lambda x: x in rec_movie_ids , relevant_movie_ids )))/len(relevant_movie_ids)\n",
    "    try:\n",
    "        f1_score = 2*presicion*recall/(presicion+recall)\n",
    "    except Exception as e:\n",
    "        f1_score = math.nan\n",
    "    return (presicion,recall,f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make testing easy, this function returns the list of the recommended movies for the given imdb_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_movie_ids(imdb_id):\n",
    "    for movie in movies:\n",
    "        if movie.id == imdb_id:\n",
    "            return movie.recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also for thesting purposes. Print the evaluation values for the given imdb_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(imdb_id):\n",
    "    for k in [1,2,3,10]:\n",
    "        metrics = evaluate_recommendations(recommend(imdb_id),get_relevant_movie_ids(imdb_id), k)\n",
    "        print(\"K = \" + str(k))\n",
    "        print(\"Presicion = \" + str(metrics[0]))\n",
    "        print(\"Recall = \" + str(metrics[1]))\n",
    "        print(\"F1 score = \" + str(metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tt2395427', 'tt2250912', 'tt0086250', 'tt0132905', 'tt0848228', 'tt0167260', 'tt3498820', 'tt0499448', 'tt2166834', 'tt0338348', 'tt0068230']\n",
      "K = 1\n",
      "Presicion = 1.0\n",
      "Recall = 0.08333333333333333\n",
      "F1 score = 0.15384615384615385\n",
      "K = 2\n",
      "Presicion = 0.5\n",
      "Recall = 0.08333333333333333\n",
      "F1 score = 0.14285714285714285\n",
      "K = 3\n",
      "Presicion = 0.3333333333333333\n",
      "Recall = 0.08333333333333333\n",
      "F1 score = 0.13333333333333333\n",
      "K = 10\n",
      "Presicion = 0.3\n",
      "Recall = 0.25\n",
      "F1 score = 0.2727272727272727\n"
     ]
    }
   ],
   "source": [
    "print(recommend(\"tt1300854\"))\n",
    "get_metrics(\"tt1300854\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
